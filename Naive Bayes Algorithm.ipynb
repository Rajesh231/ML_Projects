{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dur</th>\n",
       "      <th>Proto</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Dport</th>\n",
       "      <th>TotPkts</th>\n",
       "      <th>TotBytes</th>\n",
       "      <th>SrcBytes</th>\n",
       "      <th>Label</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>1</td>\n",
       "      <td>13485</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097566</td>\n",
       "      <td>1</td>\n",
       "      <td>35155</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>474</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3598.238525</td>\n",
       "      <td>1</td>\n",
       "      <td>43565</td>\n",
       "      <td>23</td>\n",
       "      <td>876</td>\n",
       "      <td>96819</td>\n",
       "      <td>47731</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3591.619629</td>\n",
       "      <td>1</td>\n",
       "      <td>43565</td>\n",
       "      <td>23</td>\n",
       "      <td>867</td>\n",
       "      <td>95866</td>\n",
       "      <td>47068</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1337.931763</td>\n",
       "      <td>1</td>\n",
       "      <td>44774</td>\n",
       "      <td>23</td>\n",
       "      <td>275</td>\n",
       "      <td>28434</td>\n",
       "      <td>12099</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Dur  Proto  Sport  Dport  TotPkts  TotBytes  SrcBytes  Label  State\n",
       "0     0.055572      1  13485      1        2       128        60      0      1\n",
       "1     0.097566      1  35155     10        2       474       145      0      1\n",
       "2  3598.238525      1  43565     23      876     96819     47731      0      1\n",
       "3  3591.619629      1  43565     23      867     95866     47068      0      1\n",
       "4  1337.931763      1  44774     23      275     28434     12099      0      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\Rajesh\\OneDrive\\Documents\\finalpreprocessed2.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026917408559906125\n",
      "\n",
      "\n",
      "Fraud Cases: 26885\n",
      "Valid Transactions: 998796\n"
     ]
    }
   ],
   "source": [
    "# Determine number of fraud cases in dataset \n",
    "Botnet= df[df['Label'] == 1] \n",
    "Normal = df[df['Label'] == 0] \n",
    "outlierFraction = len(Botnet)/float(len(Normal)) \n",
    "print(outlierFraction) \n",
    "print(\"\\n\")\n",
    "print('Fraud Cases: {}'.format(len(df[df['Label'] == 1]))) \n",
    "print('Valid Transactions: {}'.format(len(df[df['Label'] == 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1025681, 7)\n",
      "(1025681,)\n"
     ]
    }
   ],
   "source": [
    "# dividing the X and the Y from the dataset \n",
    "#df1 = df1.drop(['Proto'],axis =1)\n",
    "df1=df[:]\n",
    "df1 = df1.drop(['State'],axis =1)\n",
    "X = df1.drop(['Label'], axis = 1) \n",
    "y = df1[\"Label\"] \n",
    "print(X.shape) \n",
    "print(y.shape) \n",
    "# getting just the values for the sake of processing \n",
    "# (its a numpy array with no columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 0, stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.50069123e+00, -4.09003823e-01,  1.27191267e+00, ...,\n",
       "        -5.33356055e-03, -4.70500385e-03, -1.65548494e-03],\n",
       "       [ 2.95269579e+00, -4.09003823e-01, -6.08799721e-01, ...,\n",
       "        -4.26735702e-03, -4.63198144e-03, -1.61960052e-03],\n",
       "       [-4.49697919e-01, -4.09003823e-01,  8.49697198e-01, ...,\n",
       "        -5.86666232e-03, -4.69118175e-03, -1.67735734e-03],\n",
       "       ...,\n",
       "       [-4.49677298e-01,  1.94643748e+00, -1.82357793e+00, ...,\n",
       "        -4.00080614e-03, -4.22801106e-03, -1.32979114e-03],\n",
       "       [-4.49697951e-01, -4.09003823e-01,  4.01702298e-01, ...,\n",
       "        -5.86666232e-03, -4.71908674e-03, -1.67838261e-03],\n",
       "       [-4.49698002e-01, -4.09003823e-01,  2.32898603e-01, ...,\n",
       "        -5.86666232e-03, -4.71908674e-03, -1.67838261e-03]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "#creating a gaussian classifier\n",
    "\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model using the training data\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the output values\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual accuracy_score :  0.9443\n",
      "Actual recall_score :  0.9927\n",
      "Actual precision_score :  0.3190\n",
      "Actual f1_score :  0.4829\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test) \n",
    "#Let's check various metrics using sklearns inbuitl function to compare the values our custom functions generated\n",
    "from sklearn import metrics \n",
    "print(f\"Actual accuracy_score : {metrics.accuracy_score(y_test,y_pred.round()): .4f}\") \n",
    "print(f\"Actual recall_score : {metrics.recall_score(y_test,y_pred.round()): .4f}\") \n",
    "print(f\"Actual precision_score : {metrics.precision_score(y_test,y_pred.round()): .4f}\") \n",
    "print(f\"Actual f1_score : {metrics.f1_score(y_test,y_pred.round()): .4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[188367  11393]\n",
      " [    39   5338]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97    199760\n",
      "           1       0.32      0.99      0.48      5377\n",
      "\n",
      "    accuracy                           0.94    205137\n",
      "   macro avg       0.66      0.97      0.73    205137\n",
      "weighted avg       0.98      0.94      0.96    205137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred.round()))\n",
    "print(classification_report(y_test.round(),y_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = true_positives = 5338\n",
    "TN = true_negatives = 188367\n",
    "FP = false_positives = 11393\n",
    "FN = false_negatives = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flase positive rate of this model is0.0570\n"
     ]
    }
   ],
   "source": [
    "#False Positive rate\n",
    "FPR = FP/(TN+FP)\n",
    "print(\"The flase positive rate of this model is{:.4f}\".format(FPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true negative rate of this model is0.9430\n"
     ]
    }
   ],
   "source": [
    "#true negative rate\n",
    "TNR = TN/(TN+FP)\n",
    "print(\"The true negative rate of this model is{:.4f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model used is NaiveBayes classifier\n",
      "26885\n",
      "11432\n",
      "The accuracy is 0.944271389364181\n",
      "The precision is 0.31904847289462673\n",
      "The recall is 0.9927468848800446\n",
      "The F1-Score is 0.4829021168807672\n",
      "The Matthews correlation coefficient is0.5462089830280986\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the classifier \n",
    "# printing every score of the classifier \n",
    "# scoring in anything \n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "from sklearn.metrics import precision_score, recall_score \n",
    "from sklearn.metrics import f1_score, matthews_corrcoef \n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "n_outliers = len(Botnet) \n",
    "n_errors = (y_pred != y_test).sum() \n",
    "print(\"The model used is NaiveBayes classifier\")\n",
    "print(n_outliers)\n",
    "print(n_errors)\n",
    "acc = accuracy_score(y_test, y_pred) \n",
    "print(\"The accuracy is {}\".format(acc)) \n",
    "\n",
    "prec = precision_score(y_test, y_pred) \n",
    "print(\"The precision is {}\".format(prec)) \n",
    "\n",
    "rec = recall_score(y_test, y_pred) \n",
    "print(\"The recall is {}\".format(rec)) \n",
    "\n",
    "f1 = f1_score(y_test, y_pred) \n",
    "print(\"The F1-Score is {}\".format(f1)) \n",
    "\n",
    "MCC = matthews_corrcoef(y_test, y_pred) \n",
    "print(\"The Matthews correlation coefficient is{}\".format(MCC)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
